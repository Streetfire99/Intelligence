Welcome to Lesson 3 of Ethics and Law in Intelligence. Today we're continuing our deep dive into the ethical tensions that shape intelligence work. This time focusing on consent and awareness. In the previous lesson, we explored how intelligence gathering often creates tension between privacy and security. Now, we're going one step deeper. We're asking whether individuals truly understand what they're consenting to when their data is collected, and whether meaningful awareness is even possible in many intelligence contexts. We'll explore what informed consent means, how surveillance systems complicate it, and whether it's possible, or ethical, to collect information from people who don't know they're being observed. You'll also hear a case study on facial recognition in public spaces, and how it exposes some of the greatest areas of modern surveillance ethics. What is informed consent? Let's begin with the basics. What is informed consent? In ethics, informed consent means a person freely agrees to something based on a clear understanding of what's involved. It requires awareness of what's being collected, clarity about how it will be used, the freedom to say yes or no. Without coercion, you see this principle in action in medicine, research, education, and even marketing. If someone is asked to take part in a study or share sensitive personal data, they must be told what's at stake, what the risks are, and what choices they have. But in intelligence and surveillance, this principle breaks down. Most data is collected without any individual awareness, much less consent. People walk through a city full of security cameras, shop on websites that record every click, or use apps that track their location and communications. At best, they agree by accepting lengthy terms of service. They don't read, or don't fully understand. So while informed consent is a powerful ethical tool, it's one that often gets ignored, or redefined, when applied to intelligence and surveillance. Surveillance without awareness. Now let's consider the reality. Most people have no idea they're being watched. Let's take a public example. A city decides to install smart street cameras that use facial recognition. The goal is to deter crime and speed up investigations. The system runs 24 strokes 7 and checks faces against a watch list. Here's the problem. No one knows exactly who is on the watch list, how long data is kept, or whether errors are corrected. Passers-by are not asked for permission. Signs may be posted, but that's not consent. It's notice, and it's often vague. The vast majority of people are never told how often their face is scanned, whether they were flagged incorrectly, who has access to the footage. This creates what ethicists call non-consensual data environments. Places where data is harvested passively and invisibly. This doesn't just happen in authoritarian countries. It's also happening in democratic societies, corporate spaces, airports, stadiums, and shopping centers. And it raises a troubling question. If people don't even know they're part of a system, how can we say they consented? The limits of implied consent. You may have heard the term implied consent. It's the idea that by entering a space, like a store with cameras or a website with tracking, you're agreeing to be monitored. But implied consent is ethically weak when people have no real alternative. They're unaware of the extent of the monitoring. They can't opt out without giving up essential services, for instance. If the only way to book a visa appointment is through an app that collects excessive personal data, is that really a choice? What if your city's public transport requires biometric ID, or if your employer mandates constant activity tracking? In all these cases, people are nudged, or forced, into accepting terms they didn't help design and may not agree with. That's not real consent. It's compliance disguised as choice. This matters for intelligence work because ethical legitimacy rests not only on what's possible, but on what's just. If people are only agreeing because there's no other way, analysts must consider whether the system itself is fair, and whether it deserves public trust. Case study, facial recognition in public spaces. Let's bring this down to ground level with a specific case. In 2019, a major city in the UK piloted a facial recognition program in public areas. Cameras scanned thousands of faces every day, comparing them against police worklists in real time. Authorities claimed it helped spot suspects faster and increased public safety. But when the results were analyzed, researchers found serious problems. High false positive rates, especially for people of color. Poor public communication about where and when scanning occurred. Minimal independent oversight or legal checks. No clear appeals process for people wrongly flagged when citizens were asked whether they knew about the program. Most had no idea. Some who protested the cameras were themselves added to the watch list. This case illustrates the gap between technical capability and ethical readiness. Just because we can scan a face doesn't mean we should, especially when the system isn't accountable, transparent, or fair. What's more, it shows how quickly tools intended for safety can undermine consent, trust, and equality. Awareness and power. Here's a deeper issue. Awareness depends on power. People with more resources, money, education, social capital, are more likely to understand how data works and how to protect themselves. They may use privacy tools, understand legal rights, or avoid high-risk platforms. But others, especially the poor, young, elderly, or marginalized, may not even know their data is being harvested. They may live in areas with more surveillance. Use subsidized devices that share their data, or work jobs that demand constant monitoring. This creates what scholars call privacy inequality. Some people live exposed, while others shield themselves. As intelligence professionals, it's not enough to say they agreed or they should have read the terms. Ethics requires empathy and awareness of how consent isn't equally distributed. Those most affected often have the least control. That's why the burden of ethical restraint falls on institutions, not individuals. If you design or run intelligence systems, you must build in protections, even for people who don't know they need them. Ethical guidance for consent-aware intelligence. So how can intelligence professionals respect consent and awareness, even when full consent isn't possible? Here are some guiding principles. Minimize collection. Only gather what you need, and nothing more. Clarify intent. Be honest about why the data is being used, who sees it, and for how long. Limit retention. The longer data sits in storage, the more ways it can be misused. Enable opt-outs when possible. Give people a real choice, even if it's limited. Oversight and audits. Establish external review to ensure data is handled ethically and legally. Design for dignity. Build systems that assume every person matters, even if you never meet them. When intelligence agencies, tech companies, and analysts follow these principles, they not only reduce harm, they also increase legitimacy and public trust. Let's recap the key insights from this lesson. Informed consent is central to ethical decision-making, but often missing from intelligence systems. Surveillance without awareness creates invisible harms and ethical risks. Implied consent isn't meaningful when people have no real choice or understanding. Power and awareness are unevenly distributed. Those most affected are often least protected. Ethical intelligence practice requires humility, restraint, and accountability, even when the law doesn't demand it. In our next lesson, we'll explore the ethics of secrecy, whistleblowing, and accountability, and what happens when intelligence professionals face conflicts between loyalty and justice. Before you move on, please complete the quiz and review the slides, and take a moment to reflect. When have you shared data without knowing it? What would true consent look like in that moment? Thanks for listening. See you in Lesson 4.