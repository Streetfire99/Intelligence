Ok, buongiorno e benvenuti in questo modulo 4 che è il modulo 4 del corso Data Analytics e qui c'è il modulo di imparazione insuperato Cosa è questo modulo nel contesto del corso? Ok, abbiamo visto che è molto rilevante se vuoi analizzare i dati di rischio è necessario considerare approcci con i dati di uso perché i dati sono centrali nell'analisi dell'informazione che possiamo avere ma i dati in sé non sono strettamente informazioni diventano informazioni quando possiamo analizzarli quindi il primo problema che facciamo quando analizziamo i dati è definire il problema e definire cosa dobbiamo fare con i dati poi è fondamentale usare i dati per ottenere esplicitamente alcune risposte quindi abbiamo già notato che è molto importante analizzare la qualità dei dati e poi, analizzando i dati, è molto rilevante costruire alcuni modelli in modo da poter considerare i dati e trovare alcune risposte quindi la differenza è che vogliamo considerare il modello statistico e in questo senso, usando specificamente metodi statistici possiamo essere univariati, multivariati e così via e poi è possibile considerare anche approcci diversi che siano più descrittivi e esplorativi qui il contesto è diverso il contesto qui è il computational one in cui l'inferenza è basata sulla cross-validazione quindi queste tecniche sono alternative alle tecniche che abbiamo già visto nei primi tre modelli ma sono molto importanti perché permettono di considerare approcci diversi e modi diversi per rispondere alle domande che abbiamo sui dati ovviamente questi metodi possono essere usati come alternativa ma possono anche essere usati per evaluare lo stesso problema usando approcci diversi o anche per considerare un approccio di analisi sensitiva in questo senso esistono due differenti frameworks in machine learning, che è il concetto che consideriamo qui in questo corso che è un approccio di apprendimento insuperato che è un tipo di machine learning che aiuta a trovare pattern in data ma pattern in data che sono caratterizzati senza avere un label specifico quindi in questo senso vogliamo considerare non solo alcune decisioni diverse basate sui dati senza alcune ipotesi o assumazioni ma allo stesso tempo ci sono alcuni casi in cui vogliamo semplicemente osservare e classificare le osservazioni a base della loro similitudine e della loro dissimilitudine un altro approccio tipico dell'apprendimento insuperato è l'approccio di riduzione dimensionale abbiamo varie variazioni e vogliamo ridurre le variazioni in alcune variazioni speciali che combinano le informazioni che abbiamo anche i dati possono essere processati considerando i dati senza label quindi in questo senso dobbiamo considerare questi dati senza label e ridurre assolutamente i label perché i label possono essere molto pericolosi nella costruzione del modello ma anche l'identificazione dei label non è così semplice quindi dobbiamo considerare alcune tecniche statistiche che sono necessarie per pulire il nostro dataset che può creare alcuni problemi nei risultati finali in questo senso ci sono alcune procedure standardizzate e normalizzate che possono essere utili e l'ingegneria fisica può essere usata anche ma è un'altra procedure che è importante per considerare un preprocessing dell'approccio di superversale learning per ridurre la dimensione dei dati quindi in questo senso la riduzione delle dimensioni è stata usata per una procedure di clustering in cui possiamo usare la riduzione delle dimensioni come tecnica in se stessa ma anche come tecniche complementari come tecniche che possono essere usate per creare alcune nuove variazioni che possiamo usare in questo senso è molto importante che questo tipo di tecniche possa essere usata in analisi di rischio per identificare situazioni simili identificare unità statistiche simili che possono essere considerate per decisioni simili in questo senso il preprocessing dell'approccio di superversale learning sarebbe molto utile in questo contesto Statisticamente e quantitativamente le distanze sono molto importanti perchè le distanze sono la chiave al clustering le distanze permettono una misurazione di un processo che permette di capire quanto vicino o lontano i punti sono dagli altri e quindi è possibile identificare con la considerazione di distanze le differenti caratteristiche delle differenti unità statistiche in questo senso esistono diversi approcci il primo è basato su K-Means K-Means è un algoritmo è un algoritmo di clustering che viene usato nell'analisi di un set di dati in un gruppo di differenti cluster che sono definiti dai loro centri in questo senso è molto importante il K-Means perchè è possibile ridurre il numero di unità statistiche in un numero di cluster che sono il gruppo più simile di osservazioni per similitudine quindi il gruppo di osservazioni più simili è possibile trovare e le osservazioni più dissimili è possibile identificare questo è molto importante perchè in questo senso è un algoritmo che inizia con un'inizializzazione poi c'è un'update e da ogni update c'è una ripetizione dell'algoritmo fino a c'è la soluzione e la la convergenza o la creazione dei risultati in questo senso è importante che il numero del numero di cluster debba essere definito a priori e questo guida a un processo di validazione di cluster è molto diverso perchè dove K-means inizia dall'idea definiamo a priori il numero di cluster il clustering gerarchico permette di creare una gerarchia di diversi cluster quindi usando un dendogramma che può essere ottenuto dall'algoritmo per ottenere una similità e una visualizzazione della struttura della variabile della differente osservazione in questo senso il processo continua fino a ogni datapunto è in un cluster individuo e la visualizzazione è fatta usando un dendogramma in ogni passaggio la validazione del clustering è il processo di evaluazione della qualità e dell'effettività di un algoritmo di clustering e la validazione del clustering è importante perchè può essere ottimizzata considerando alcuni approcci per assessare la compatibilità in particolare ci sono due approcci che sono la validazione esterna o la validazione relativa in particolare la validazione esterna è basata su una verità fondamentale quindi risultati specifici che sono necessari per comparare i risultati ottenuti dal clustering con la validazione relativa per determinare il miglior approccio per un dataset in particolare poi un'approccio molto diverso è il studio supervisivo dove il studio non supervisivo parte da differenti unità statistiche senza alcun label qui abbiamo un label e in questo senso è possibile allenare l'algoritmo per predicare

per predicare l'observazione labellata e poi, usando il modello, il modello trattato, è possibile provare a predicare nuove osservazioni. In questo senso, è possibile osservare che dove l'apprendimento insuperato è basato sul clustering e la classificazione delle osservazioni, qui, è più probabile che sia relazionato a un problema di predicazione, quindi vogliamo predicare diverse osservazioni dove abbiamo già qualcuno con cui possiamo trattare esplicitamente il modello, in particolare, per superare gli errori dell'algoritmo, trovando patterni nel dato, e cercando di misurare le entrate per considerare alcune altre osservazioni che sono collezionate in un set di test. Per esempio, questa approccia può essere usata per filtrare spam, per riconoscere immagini, può essere usata anche per la riscanalizzo, è possibile anche considerare applicazioni basate sulle predicazioni dei modelli, basate su altri tipi di dati di rischio, che possono essere molto importanti. Quindi, in questo senso, la predicazione dell'analisi, che è considerata l'apprendimento insuperato, è molto importante. Quindi, gli outliers sono molto rilevanti in questo contesto, perché gli outliers possono creare alcuni problemi nel predicare, e quindi gli outliers devono essere identificati, è necessario minimizzare i diversi outliers che esistono. Inoltre, i dati scassati sono molto generali, e in questo senso è possibile sostituire gli outliers, perché i dati scassati sono frequenti, ma è necessario considerare le tecniche usate per analizzare se i dati scassati non devono essere considerati o imputati, ma allo stesso tempo è possibile sostituire gli outliers con i mediani o i significati del dataset, perché gli outliers possono essere pericolosi anche in questo senso, quindi, dove e quando i dati specifici sono imputati esplicitamente per considerare questo tipo di approccio, è possibile considerare un metodo di regressione in passato. Qui, consideriamo tipicamente in passato il dato logistico, che è l'applicazione più tipica in cui possiamo considerare questo tipo di dato, questo approccio, ma questo approccio può anche essere applicato a dati continui, considerando in passato la regressione. In questo senso, è un metodo in supervised learning per eliminare i vari predictors che non sono rilevanti, quindi vogliamo scegliere un subset di caratteristiche rilevanti, ma sistematicamente aggiungendo o rimuovendo predictors. In questo senso, la regressione logistica è applicata ad un modello in cui la variazione dipendente è categorica e spesso binaria, dove c'è un numero molto alto, un numero enorme di variazioni, e dobbiamo decidere quali sono i più rilevanti. In questo senso, inizializziamo con un modello iniziale che può esplodere e poi eliminiamo o aggiungiamo nuove variazioni quando finalmente ci sono risultati specifici basati su tutte le variazioni più rilevanti e più predittive e tutte queste variazioni predittive fanno parte del modello finale, quindi per ogni passaggio aggiungiamo e eliminiamo alcune variazioni, evaluiamo i diversi risultati e poi i risultati finali, i risultati che ottimizzano la buoness of fit del modello considerando i criteri di informazione archaiche, o i criteri di informazione biase, o l'area squarata o l'area adesso squarata, e quindi terminiamo se aggiungiamo o eliminiamo variazioni che non significativamente migliorano il modello in questo senso la cross-validazione è ciò che tipicamente facciamo, in pratica la cross-validazione divide il setto specifico di dati in due, quindi dividiamo il dataset in un 80% di osservazioni del set di allenamenti e 20% del set di test e le osservazioni sono assaggiate randommente sui diversi set specifici, in questo senso operiamo sul set di allenamenti specificamente mettiamo l'algoritmo in funzione nel set di test, nel set di allenamenti e poi appliciamo il modello costruito sul set di test, quindi il dato, l'algoritmo è estratto e poi l'approccio è usato usando il set di test, questa è la cross-validazione e la cross-validazione aiuta a detectare le possibili sovrapposizioni e assicurare che ci sia una molto buona generalizzazione dei risultati, quindi in questo senso esistono diversi algoritmi che si possono usare nell'apprendimento superviso e ciò più importante sono i network neural, in cui consideriamo alcuni nodi interconnettati che si chiamano neural che possono lavorare insieme per trasformare un input in un output e di solito possono essere un lavoro molto buono in un modo prevedibile, ma allo stesso tempo usiamo la cross-validazione in modo da trovare i risultati finali. Un altro approccio è la decision tree, in cui è usato dividere le date in branche a base di criteri specifici e poi anche loro sono molto buoni per prendere in contatto gli outliers e non creare problemi in questo modo se ci sono problemi tipici di data in questo senso. Il tuning dei parametri è relato al fatto che puoi cambiare i diversi parametri in modo da ottimizzare i parametri del modello di apprendimento della macchina, quindi i parametri del modello che possono essere collegati all'uso dei diversi algoritmi possono essere cambiati in modo da trovare i risultati migliori e quindi in questo senso è possibile usare diversi algoritmi o diversi comparatori, diversi risultati, per esempio è possibile usare una ricerca di algoritmi e una ricerca di random per trovare i parametri che ottimizzano i risultati differenti predittivi e quindi in questo senso è possibile considerare questi approcci in modo da ottimizzare qualsiasi parametro prima di usare questi modi, questi approcci in produzione, voglio dire, in un buon modello predittivo. L'apprendimento del modello è basato su un altro fatto, il fatto che possiamo considerare diversi algoritmi perché esistono diversi algoritmi predittivi ma allo stesso tempo ci sono diversi algoritmi clustering e quindi consideriamo questo tipo di diversi algoritmi in modo da avere risultati diversi che sono votati dai diversi algoritmi e poi usando una regola maggiore o altre regole è possibile aumentare la robustezza dei risultati diversi possiamo essere più robusti. In questo senso abbiamo esplorato l'apprendimento della macchina e l'apprendimento dei supervisor in modo da trovare i migliori approcci nella classificazione e nel clustering senza alcun label, quindi abbiamo prima di tutto preso conto di un approccio che è basato sul fatto che vogliamo esplicitamente dividere il nostro dataset nei gruppi più rilevanti e questo è un elemento molto importante perché a volte in rischio di analisi dobbiamo esplicitamente classificare i nostri dati, ma allo stesso tempo è importante definire un modello che può essere considerato un modello predittivo, quindi in questo senso un approccio che potrebbe essere usando l'apprendimento dei supervisor in modo da trovare da un dataset labellato un algoritmo che è allenato e che può predire bene e che può essere applicato a un altro set di dati che è il test set. In questo senso questi algoritmi potrebbero essere molto potenti per risolvere problemi che esistono nell'analisi di rischio. Grazie mille per l'attenzione