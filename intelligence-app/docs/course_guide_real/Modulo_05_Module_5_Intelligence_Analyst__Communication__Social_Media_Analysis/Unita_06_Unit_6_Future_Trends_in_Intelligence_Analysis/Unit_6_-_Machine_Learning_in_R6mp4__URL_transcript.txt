Buongiorno e benvenuti nel 6° modulo, Maschine Learning in R in cui discuteremo l'uso di Maschine Learning direttamente in R e vedremo in quale modo è usato il Maschine Learning in R in questo senso continueremo i metodi che abbiamo già visto, i metodi computazionali che abbiamo visto nel modulo precedente e in particolare vedremo ora un approccio diverso, un framework diverso nel quale considereremo il Maschine Learning in questo senso il Maschine Learning è una metodologia che è molto importante perché permette di eseguire e risolvere problemi importanti che possono succedere sui dati in realtà possono succedere in generale e possono essere risolviti usando i dati e in questo senso il termine di imparazione è basato sul fatto che impariamo dai dati quindi il Maschine Learning è una metodologia in cui impariamo dai dati e risolviamo problemi direttamente con questo imparamento in ogni caso iniziamo ora a definire in R due approcci diversi approcci come l'approccio di imparazione insuperata che è relato al fatto che possiamo classificare esplicitamente alcuni datassetti o meglio, classifichiamo diverse osservazioni nei nostri dati e in questo senso vogliamo gruppare i punti di dato in un cluster a base delle loro similitudini allo stesso tempo nell'approccio di imparazione insuperata c'è l'analisi principale del componente che è usato per la riduzione delle dimensioni e può essere usato per semplificare i dati in modo da trovare variazioni relevanti che possano essere considerate quindi in questo senso le due approcci che sono tipiche nell'apparazione insuperata sono l'analisi del cluster e l'analisi del componente principale in questo senso in R un problema rilevante è iniziare ovviamente dal preprocessing che può essere usato per l'apprezzamento insuperato in questo senso è importante preprocessare i dati per arrivare a una definizione specifica di un tipo di data più specifico in questo senso ci sono molti altri approcci che possono essere considerati che sono molto simili ai diversi problemi ma l'emphasis è diversa per esempio nell'apprezzamento insuperato la deteczione dell'outlier è molto importante perché in questo caso i modelli predittivi possono essere affettuati negativamente dall'uso di dati non corretti quindi in questo senso è molto importante preprocessare i dati in modo da identificare le migliori soluzioni in questo senso le distanze sono importanti perché permette la procedura di clustering in questo senso è possibile identificare e classificare i diversi dati in modo da identificare gli elementi specifici che possono essere usati nel clustering in questo senso dobbiamo capire le distanze e la similità tra i punti di dato ogni metodo di clustering usa distanze, distanze diverse in questo senso le distanze sono molto importanti perché permettono di fare il clustering correttamente in questo senso l'uso delle distanze è all'interno di ogni approccio di clustering in questo senso è all'interno della metodologia e non solo una tasca di computazione ma considerando le diverse metodologie è possibile usare le distanze in modo da trovare qualcosa che possiamo considerare esplicitamente in questo senso è possibile usare le distanze diverse come la distanza Euclidea, Manhattan, Minkowski, Cosine e Jacquard la distanza Euclidea è la più relevante e la più usata che è usata per esempio in K-Means ma è anche usata in dati da analisi principali dei componenti o altri metodi di riduzione multidimensionale in cui i dati hanno la stessa scala e sono tipicamente ottenuti da un processo di standardizzazione in questo senso le distanze sono parte del metodo di clustering che può essere applicato in R ci sono diversi metodi di clustering che possono essere applicati in R il primo è K-Means che è un'algoritmo di clustering popolare che è usato in analisi di dati in questo senso l'idea è di particellare i dati quindi il risultato non è un'erarchia ma è tipicamente una particellazione dei dati in diversi cluster in questo senso K-Means è un'algoritmo di clustering molto usato per creare alcuni gruppi che sono ottenuti da un procedimento iterativo in questo senso è importante scegliere direttamente il numero di K, il numero di diversi gruppi o cluster e poi l'algoritmo può ritornare le diverse osservazioni che sono considerate in ogni specifico cluster e quindi l'output ritorna i diversi cluster, i membri, i centri e così via è anche importante visualizzare i diversi clustering per considerare gli elementi specifici che sono ritornati dal clustering l'evaluazione del clustering è importante, quindi un'importante questione è la validazione del cluster perché ovviamente il numero di cluster può non essere corretto quindi è possibile ottimizzare la struttura del cluster per trovare gli elementi che sono molto importanti per considerare i cluster quindi in questo senso è necessario assessare la buonessa del cluster ottenuto da un processo di validazione e in questo processo di validazione c'è una procedura molto rilevante che possiamo vedere nel codice dove la funzione specifica per realizzare K-means è K-means il clustering di hierarchia proviene da un problema diverso, che è creare una hierarchia di cluster considerando un approccio agglomerativo o divisivo in questo senso il clustering di hierarchia è un metodo statisticale che gruppa oggetti simili into cluster creando anche una hierarchia dei dati la hierarchia dei dati è basata su uno strumento molto tipico ottenuto strumento e anche oggetto o grafo o output, che è il dendogramma il dendogramma è un tipo di visualizzazione che aiuta a decidere il numero di cluster per tagliare into tre a un livello specifico usando le funzioni Q3 quindi HCluster è una funzione per ottenere un dendogramma e K3 è una funzione per assignare ogni oggetto a una cluster in questo senso, l'apprendimento superviso riferisce a metodi macerie in cui è un approccio molto diverso dall'apprendimento non superviso perché l'apprendimento non superviso l'abbiamo già visto considerando il clustering di hierarchia e K-means che sono basati sulla stessa idea della validazione perché sono basati sul fatto che vogliamo massimizzare la compattità e la buonessa dei diversi cluster ottenuti ma l'apprendimento superviso è radicalmente diverso perché consideriamo un approccio che riferisce a allenare un algoritmo diverso con alcuni dati e poi testare i dati considerando alcune risposte che sono conosciute quindi abbiamo label in l'apprendimento superviso mentre nell'apprendimento non superviso non abbiamo apprendimento quindi identifichiamo alcuni pattini direttamente dai dati o l'algoritmo comprende alcuni dati dai pattini e è possibile anche usare le differenze

differenti algoritmi di treno per predicare i dati che non sono considerati. In questo senso, il preprocessing è importante, perché permette di evitare problemi come i dati scomparsi, ma allo stesso tempo è molto importante considerare il fatto che non possiamo esistere in questo senso potrebbero creare alcuni problemi, perché potrebbero inserire alcuni rumori nel modello predittivo e i parametri potrebbero essere assolutamente non buoni. In questo senso, è possibile eseguire l'apprendimento supervisivo seguendo diversi passi. Il primo passo è l'identificazione dell'outlier, in cui consideriamo metodi specifici che saranno utili per identificare gli outliers e poi è possibile considerare la sostituzione dei diversi outliers o la riduzione. La riduzione è importante perché, a volte, gli outliers possono essere veri, le osservazioni possono anche essere, allo stesso tempo, alcune osservazioni che sono erronee, in questo senso, quindi, in questo senso, è possibile considerare i diversi outliers e interpretare se sono buone osservazioni o no. In questo senso, trasformare e normalizzare gli outliers è importante perché è molto importante minimizzare l'impatto degli outliers e, in questo senso, costruire un modello robusto è molto importante per identificare i metodi specifici in modo da migliorare e ottimizzare i risultati del modello predittivo. Un metodo molto importante per migliorare il modello predittivo è la regressione in passato, che può essere implementata in ART. La regressione in passato può essere implementata sia per i dati continui, in questo senso, usiamo un modello di regressione lineare qui, che è usato nell'apprendimento della macchina con l'obiettivo predittivo, dove, allo stesso tempo, è possibile costruire una regressione in passato in un modello di regressione logistica. In questo senso, siamo obiettivi a ridurre il modello identificando i migliori predittori possibili da trovare. In questo senso, abbiamo molti, molti, molti differenti predittori e non in ogni caso l'ingegneria fisica, come la creazione di un singolo indicatore da un componente del PCA, è una fine. In questo senso, è possibile considerare un approccio specifico per identificare i diversi dati. Quindi, in questo senso, l'approccio migliore sarebbe qui per identificare la migliore variazione per ottimizzare il modello considerando una regressione in passato. La regressione in passato è basata sulla preparazione dei dati, l'aspetto del modello iniziale, poi usare la selezione in passato per identificare le migliori variazioni specifiche con una procedura in cui ogni volta sono aggiunte e rimuovete alcune variazioni per esplorare la soluzione ottenuta e la soluzione, la buona soluzione, è la soluzione che ottimizza alcune selezioni, l'approccio di selezione del modello, come ad esempio i criteri di informazione archaica o i criteri di informazione biase, o anche la regressione logistica in passato o la regressione in passato in questo senso, la regressione logistica in passato o la regressione in passato è in grado di validare il modello che può aiutare a identificare le variazioni specifiche che possono essere utilizzate nel modello predittivo quindi l'idea di parsimonia è migliore di ridurre l'overfittimento del modello evitando il bias o l'overfittimento trovando il modello che ottimizza la performance predittiva allo stesso tempo, la cross-validazione può essere implementata in AR ottenendo il 80% delle osservazioni e il 20% del test le osservazioni sono distribuite in random tra i sub-set e possono permettere l'assessore non biaso del modello in questo senso, la logica è che usiamo il 80% per considerare i migliori parametri e considerare i migliori variabili poi il test-set permette di validare il modello in questo caso, la validazione è basata sul test-set il test-set, ovviamente, non è basato su osservazioni specifiche ma è basato su osservazioni nuove non viste in effetti, in questo senso, tutte le osservazioni conosciute in allenamento non sono considerate nel test-set e nel test-set simuliamo esplicitamente l'uso di un modello per data nuove non viste allo stesso tempo, gli algoritmi di imparazione superiore possono essere considerati molto diversi come il Capanere's Neighbor, Random Forest, Regression, Decision Tree, e così via e ognuno può essere usato in R Network, ad esempio, è molto semplice considerato in R e può essere costruito in R ma allo stesso tempo è possibile costruire altri algoritmi di imparazione diversi un altro elemento importante è il tuning in R, che permette di esplorare la lunghezza delle possibili soluzioni e esplorare allo stesso tempo le soluzioni attraverso i parametri, quindi vogliamo trovare i parametri che ottimizzino la nostra soluzione e quindi in questo senso dobbiamo trovare la migliore parametrizzazione e quindi in questo senso ottimizziamo la struttura del modello predittivo che usiamo ci sono molte strategie diverse in R che possono essere usate, creando una ricerca grida, una ricerca randomizzata e un'ottimizzazione biasa e quindi in questo senso la valutazione del modello si permette iterando i modelli diversi quindi dobbiamo iterare molto tempo e molti rondi i risultati diversi per trovare i risultati diversi che sono i migliori un altro approccio diverso è l'esercizio di insegno in R, che è semplicemente creato considerando gli algoritmi diversi e considerando i risultati diversi per ogni algoritmo diverso ad esempio, in vari casi ogni algoritmo, ogni algoritmo diverso in R può darci una soluzione diversa e possiamo creare una regola maggiore nella quale la predizione finale è determinata selezionando la classe dei risultati più frequenti prediti da tutti gli algoritmi individuali in questo senso, il metodo può aumentare la rilevanza e l'accurazione dei risultati con l'integrazione di molti algoritmi in questo caso, l'esercizio di insegno di macchine è un approccio molto rilevante e può essere usato o possiamo considerarlo in R che è un ottimo strumento per non solo considerare l'analisi di dati classiche ma allo stesso tempo, l'analisi di dati moderne basata sull'insegno di macchine l'analisi di dati, in questo senso, può essere vista come una grande scatola di metodologie diverse in cui possiamo scegliere la metodologia migliore considerando il problema e considerando cosa la metodologia può fare specificamente per risolvere il problema nell'analisi di rischi in questo senso, se abbiamo bisogno di clusterare alcune osservazioni possiamo usare un'approccio diverso nell'insegno di macchine o un'approccio diverso nell'analisi di dati ma allo stesso tempo, se vogliamo predire possiamo usare l'inferenza classica o l'insegno di macchine quando il problema ci permette di farlo in tutti i casi c'è la scelta di diversi software, di diversi linguaggi di programma che sono uno dei possibili da usare può essere molto interessante perché è molto flessibile e è molto capace di mescolare analisi diverse grazie per l'attenzione