Ok, buongiorno e benvenuti nel modulo 2, che è Processing and Data Exploration. Questo modulo è il secondo modulo del corso in Data Analytics for Risk Analysis, in cui l'idea è che vogliamo analizzare i dati relati ai rischi in diversi paesi, in diversi negozi, in diverse situazioni. In questo senso dobbiamo partire dal fatto che abbiamo una matrica di dati e abbiamo già visto nel primo modulo il modo di arrivare a una matrica di dati. Perciò, prima di tutto, definiamo un problema, una domanda specifica, che è la domanda che vogliamo considerare nell'analisi, e poi arriviamo a definire l'analisi, considerando i dati, considerando una collezione di dati valida, ma allo stesso tempo una metodologia statistica o quantitativa rilevante. E in questo senso possiamo usare approcci assolutamente diversi, che possono essere usati per risolvere il problema. Poi abbiamo considerato la metodologia, che è esattamente ciò che facciamo in questo modulo, e poi analizziamo i diversi risultati, anche considerando l'analisi di sensibilità, per ottenere diversi risultati. Quindi, il scopo dell'analisi di dati è risolvere problemi nell'analisi di rischi e nell'analisi di diversi negozi, paesi, e così via, per risolvere problemi relati all'intelligenza e per ottenere alcuni consigli che possono essere utili per la decisione. In questo senso, questo è il scopo di questo secondo modulo. In effetti, questo modulo introduce tre topici rilevanti, che sono il preprocessing di dati, la descrizione di dati e l'esplorazione di dati. Questi due moduli sono, insieme, importanti, perché introducendo un'analisi più profonda, possiamo permettere di costruire un modulo. E quindi è molto importante qui definire qual è la differenza tra costruire un modulo e avere un'esplorazione di dati e un'analisi di dati. Costruire un modulo significa che vogliamo creare una rappresentazione della realtà, quindi dobbiamo considerare una sorta di realtà stilizzata da dati, e poi questa mappa della realtà non dovrebbe essere così grande, e quindi dovrebbe dimostrare ciò che è davvero importante. L'esplorazione di dati è diversa, perché il modulo può essere utilizzato per fare predizioni, e quindi stiamo considerando il modello di dati per creare predizioni, l'esplorazione di dati è davvero diversa, perché permette di esplorare i dati e trovare alcune evidenze empiriche direttamente sui dati. Ovviamente un problema potrebbe essere significativo, e in effetti l'analisi empirica non dovrebbe essere così significativa, perché può essere random. Un problema tipico quando otteniamo alcuni risultati, è che dobbiamo considerare se i risultati sono random, o se sono basati su un'evidenza specifica, probabilistica, che permette di fare alcune predizioni per il futuro. In ogni approccio c'è la rilevanza del preprocesso di dati. Il preprocesso di dati è fondamentale perché è fondamentale per gestire diversi problemi che possono essere presenti sui dati, ma allo stesso tempo il preprocesso di dati è molto importante per ottenere una pictura di dati rilevante che possiamo usare per l'analisi. In questo senso, il preprocesso di dati usa tecniche specifiche, che sono tecniche specifiche di analisi di dati, analitica descrittiva, che sono molto importanti per definire esplicitamente una struttura sui dati. L'esplorazione è molto diversa, e l'esplorazione è utile per identificare la struttura dei dati, identificare quali strutture sono possibili trovare sui dati, e in generale è molto importante per ottenere conoscenze sui dati. Ma allo stesso tempo l'esplorazione è utile anche per capire la qualità dei dati, quindi è molto importante identificare se i dati sono alti in qualità o no. In questo senso, ci sono casi in cui i dati possono essere caratterizzati da elementi specifici e rilevanti, come dati scoperti, dati con errori, outliers e così via. Cosa significa la qualità dei dati in generale? La qualità dei dati è una caratteristica dei nostri dati, che non è specificamente assicurata, ma deve essere controllata, perché nell'analisi di rischi e in generale nei dati che sono raccolti da diverse fonti, è tipico di avere alcuni problemi, come data scoperte, inconsistenze, come dati con errori, outliers e così via. E quindi potrebbe essere un problema, perché questo tipo di dati può guidare a un bias, e quindi questo problema deve essere gestito e considerato prima dell'analisi. In questo senso, è un risultato tipico che la qualità dei dati può determinare la realizzabilità e la validità dell'analisi, e quindi l'annullamento della qualità dei dati è molto rilevante per trovare risultati specifici, che sono molto rilevanti. Allo stesso tempo ci sono i dati scoperti, che sono altre caratteristiche tipiche dei dati reali, i dati scoperti sono osservazioni tipiche, che non hanno un valore specifico, e le osservazioni possono essere tipicamente dipendenti da un fenomeno o da altre variazioni, e quindi è necessario trovare quello che è il patterno specifico e rilevante che crea e genera le diverse inconsistenze. Allo stesso tempo, se è possibile trovare dati specificamente random, possiamo trovare dei dati scoperti a random. Allo stesso tempo, in questo senso, i metodi statistici non sono disegnati per lavorare con i dati scoperti, e quindi è molto importante identificare se è possibile eliminare, eliminare il valore scoperto, o se è necessario gestire questi dati, ad esempio per un'imputazione. L'imputazione è possibile fare in diversi modi. La prima, la più semplice, è creare un avvero, dove l'avero è anche un approccio rischio, perché dove possiamo considerare, ad esempio, i dati cross-sectionali, un avero può creare un avero tra i dati diversi, che sono davvero, davvero diversi. In questo senso, è molto importante imputare i dati considerando il più simile. Un primo problema importante che è necessario dire è che l'imputazione, in questo senso, può essere fatta considerando una strategia di imputazione, ma allo stesso tempo è molto importante considerare una strategia possibile che imputa i dati specificamente usando un approccio che imputa il valore più probabile considerando la struttura dei dati. Per esempio, è possibile imputare i dati usando un primo modello che predica le variazioni, come una variazione dipendente, e poi usare questo modello per l'imputazione delle osservazioni. Inoltre, ci sono possibilità di errori. Errori dei dati possono guidare allo stesso tempo ad un'analisi inconsistente e è possibile considerare errori che sono inconsistenze dei dati dove è possibile e necessario correggere i dati se possibile. Errori sono il terzo tipo di problemi possibili nei dati e errori possono anche affettare l'accurazione e la consistenza dell'analisi. In questo senso, è un'espressione tipica che spiega il fatto che è possibile ottenere un biase nei risultati dipendendo dai errori. In questo senso, è importante identificare, gestire e correggere gli errori in modo da assicurare la validità dell'analisi. Gli errori sono un'altra questione rilevante sui dati e gli errori dipendono dal fatto che ci sono alcuni valori che sono più alti o più bassi o molto alti o molto bassi.

e sono esplicitamente basate, possibilmente, su error, come alcuni problemi che possiamo considerare un dato falso, ad esempio. In questo senso, è necessario ripetere l'analisi con e senza lierare, ad esempio, con un approccio di sensibilità di analisi, o è possibile considerare un'imputazione di questo tipo di dati. Poi, dalla qualità del dato alla preprocessione del dato. La preprocessione del dato, in generale, è basata su diversi approcci, anche la analisi del dato è basata sulla qualità del dato, e quindi l'analisi del dato è basata sulla preprocessione del dato, ma l'analisi del dato è un'altra cosa importante che è necessaria per fare sui dati e l'analisi del dato ci mostra perché è importante la preprocessione del dato, perché ci sono molte operazioni specifiche che sono importanti, quindi la preprocessione del dato è molto importante quando conduciamo l'analisi del dato e quindi è molto rilevante operare in questo senso. La ripetizione è importante, è utile anche se ci sono modalità che non sono sufficienti per esprimere alcune caratteristiche reali rilevanti, quindi in questo senso vogliamo ripetere i dati. Per esempio, vogliamo considerare 4 classi o 3 classi su un singolo fenomeno e quindi in questo senso è necessario ripetere le variazioni. Anche è possibile trasformare alcune variazioni textuali in variazioni qualitative, quindi in questo senso è necessario trasformare alcune variazioni textuali in variazioni qualitative. La trasformazione tipica è la normalizzazione tipica che permette al dato di aderirsi in una linea specifica e questa misura assicura che tutte le caratteristiche possano contribuire ugualmente, soprattutto quando si tratta di dati da scala diversa. La standardizzazione, inoltre, implica la riscalazione dei dati, considerandoli 0 e una deviazione standard di 1. Inoltre, questa approccia è utile per standardizzare variazioni diverse. Un'altra trasformazione è la trasformazione logaritmica che permette di stabilire le varianze per i dati da scala. L'ingegneria fisica è diversa dal preprocessing del dato perché permette di combinare e di ottenere una singola variazione considerando diverse variazioni. In questo senso, abbiamo diverse variazioni che dobbiamo combinare in variazioni uniche, quando si tratta di un'analisi principale dei componenti, un'altra approccia è la composizione dei indicatori. E' importante perché, in molti casi, usare troppe variazioni potrebbe non essere buona, e quindi otteniamo un miglioramento della performance del modello attraverso la scelta attenta e la trasformazione delle caratteristiche in una. L'esplorazione del dato è importante perché dobbiamo, dal preprocessing del dato e dal problema di identificazione della qualità del dato, l'esplorazione del dato è importante per identificare quali sono i problemi. Ma, e questo è molto importante, l'esplorazione del dato è possibile da sola. In questo senso, l'esplorazione del dato è importante perché, tipicamente, non abbiamo bisogno di confermare alcune ipotesi e vogliamo solo analizzare il dato con l'obiettivo di scoprire alcuni consigli relevanti nel dato. In questo senso, gli obiettivi rilevanti sono scoprire la struttura del dato, identificare gli applicatori rilevanti e assicurare assolutamente la ragionabilità delle ipotesi dell'analitico metodo usato. È molto importante che l'esplorazione del dato debba essere fatta dopo aver detto e specificato l'ipotesi, che è il primo elemento da studiare. Poi, l'esplorazione del dato, in modo da evaluare la struttura del dato. Poi, l'analisi scrittica è importante perché permette di identificare alcuni elementi rilevanti dal dato, che sono l'analisi del mezzo, la varianza, la deviazione standard, il minimo e il massimo, come elementi rilevanti che possiamo osservare nel dato, che possiamo comparare. Il minimo è la locazione centrale del dato, dove la varianza misura la variazione e la deviazione standard. Il minimo e il massimo per ogni variazione rappresenta il minimo e il massimo del raggio che possiamo considerare. A volte, e forse molto spesso, è importante anche descrivere l'analisi di data bivariate e multivariate. Bivariate si tratta di relazioni tra variazioni diverse, mentre l'analisi di data multivariate si tratta del fatto che vogliamo analizzare molte variazioni diverse. In questo senso, l'analisi di molte variazioni deve essere molto importante, perché possiamo considerare molte variazioni, e in questo senso vogliamo considerare cosa succederà in molti casi se definiamo una variazione, come il target, che dipende da altre variazioni, e altre variazioni possono permettere di esplicare, o meglio, di prevedere queste variazioni. Questo è l'obiettivo tipico dell'analisi di data multivariate o delle statistiche multivariate. La visualizzazione di dati è una parte rilevante dell'esplorazione di dati, quindi ci sono elementi chiari che hanno in comune con le statistiche grafiche, l'analisi di dati e l'esplorazione di dati. In questo senso, ci sono alcuni principi di visualizzazione che non possono essere sottovalutati. E perché è importante la visualizzazione di dati? Perché in molti casi è molto importante capire la struttura dei dati, capire il trattamento dei dati, capire ciò che i dati specificamente dicono a noi. Qual è il problema? Il problema è che in molti casi ci può essere una forte subiettività nelle tecniche di visualizzazione, ma anche nelle tecniche di esplorazione di dati. In questo senso, l'inferenza statistica è molto importante perché ci dà assolutamente l'idea obiettiva di ciò che abbiamo da alcuni dati. Allo stesso tempo, alcuni principi di visualizzazione devono essere considerati. Scegliere le scale appropriate, evitare di scegliere anche il tipo di cartella giusta e assicurare l'accurazza, e anche evitare qualsiasi biase nella visualizzazione dei dati. In questo senso, è molto importante identificare il giusto grado senza distorsionare o biassare nessun elemento nella visualizzazione. La visualizzazione e la distribuzione sono importanti con diversi plot, boxplot, histogram, binplot, validplot, ragplot, e in questo senso è molto importante identificare la struttura dei dati in cui è importante anche identificare la relazione variabile usando plot scattati in due dimensioni, plot scattati in tre dimensioni e plot scattati in matrice. Allo stesso tempo, la massimizzazione del ratio dei dati all'inc è un'ottimizzazione molto relevante della visualizzazione dei dati. In questo senso, è importante ottimizzare il numero di osservazioni visualizzate, ma allo stesso tempo è necessario eliminare l'inc non-dati. Quindi, dove ci sono alcuni elementi che non sono importanti nella visualizzazione, devono essere evitati. Ma allo stesso tempo, l'ottimizzazione della visualizzazione dei dati ha bisogno di mostrare la densità dei dati e la clarezza, quindi le grandi quantità di dati devono essere considerate in un piccolo spazio, e anche l'incoraggio della visualizzazione. Quindi, metodi statistici, come per esempio la linea di regolazione e metodi non-parametrici, possono essere utili per visualizzare la struttura dei dati. E l'evitazione della distorsione può essere considerata. Quindi la visualizzazione non deve essere sbagliata e l'accesso deve essere chiaramente labellato e la proporzione rispettata e la trasformazione tipicamente indicata. In questo senso, la visualizzazione può essere importante perché la storia della visualizzazione può mostrare la struttura dei dati che possiamo considerare e poi, da questo approccio, è possibile identificare ciò che possiamo ottenere dai nostri dati come informazione. In questo modulo abbiamo definito due concetti importanti. Il primo concetto è l'importanza della qualità dei dati, che è molto rilevante. Un secondo elemento è la rilevanza del preprocessing dei dati, che è molto importante perché, in molti casi, quando consideriamo dei dati, dobbiamo considerare specificamente il problema che possiamo incontrare sui diversi dati. Il terzo elemento è il metodo che possiamo considerare, come l'esplorazione di dati, l'analisi descrittiva di dati, ma anche le statistiche grafiche per identificare alcuni consigli sui dati. Ci sono tecniche molto potenti che possono permettere di identificare un patterno importante sui dati. Il problema è che sono tipicamente soggettivi o c'è una soggettività in alcune conclusioni che possono essere considerate nell'esplorazione di dati, nella visualizzazione di dati, e quindi è importante considerare alcuni testi statistici in modo da identificare alcuni patterni specifici che possono essere anche statisticalmente significativi.